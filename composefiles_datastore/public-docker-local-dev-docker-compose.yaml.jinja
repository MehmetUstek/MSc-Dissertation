{{ "# THIS docker-compose.yaml FILE IS AUTOGENERATED BY THE environment-update.sh script" }}
{{ "# you should update by modifying environment-config.yaml and re-running environment-update.sh" }}

version: "3.9"

services:
{% if "webserver" in in_docker %}
  webserver:
    image: rmelickvida/public-example-django:e2cbe80
    ports:
      - "${WEBSERVER_EXTERNAL_PORT}:${WEBSERVER_INTERNAL_PORT}"
    # gunicorn still seems to log users out frequently, so run manage.py
    #command: "gunicorn vida.wsgi -c gunicorn_config.py --log-file - --access-logfile - --timeout 300 --graceful-timeout 90 --keep-alive 700 --preload --workers=2 --worker-class=gevent --bind 0.0.0.0:${WEBSERVER_INTERNAL_PORT} --log-level debug --forwarded-allow-ips * --proxy-allow-from *"
    command: ["/bin/sh", "-c", "./manage.py migrate && ddtrace-run manage.py runserver --noreload 0.0.0.0:${WEBSERVER_INTERNAL_PORT}"]
    depends_on:
      rabbitmq:
        condition: service_started
      postgres:
        condition: service_started
    environment:
      - DD_AGENT_HOST=datadog
      - DD_ENV=public-docker-local-dev
      - DD_LOGS_INJECTION=true
      - DD_SERVICE=web-service
      - DB_USER=postgres
      - DB_PASSWORD=password
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=db
      - "CELERY_BROKER_URL=pyamqp://${SERVICE1_RABBIT_MQ_USER}:${SERVICE1_RABBIT_MQ_PASSWORD}@rabbitmq/${SERVICE1_RABBIT_MQ_VHOST}"
      - DJANGO_SETTINGS_MODULE=tutorial.settings
      - SNOWPLOW_HOST=snowplow
      - SNOWPLOW_PORT=9090
      - "REDIS_CACHE_LOCATION=redis:6379"
      - CELERY_TASK_ALWAYS_EAGER=${WEBSERVER_CELERY_TASK_ALWAYS_EAGER}
      # the following are pulled from the docker .env file
      - SNOWPLOW_ENABLED
      - DD_TRACE_ENABLED
      - REDIS_PASSWORD
      - DD_API_KEY
      - SPLIT_API_KEY
      - SPLIT_REDIS_HOST
      - SPLIT_REDIS_PORT
      - SPLIT_REDIS_PASSWORD
{% endif %}

{% if "webserver-celery" in in_docker %}
  webserver-celery:
    image: rmelickvida/public-example-django:e2cbe80
    command: "celery -A tutorial worker --loglevel=DEBUG -Q celery --without-mingle --without-gossip --max-tasks-per-child=500 --concurrency 1"
    depends_on:
      rabbitmq:
        condition: service_started
      postgres:
        condition: service_started
    environment:
      - DD_AGENT_HOST=datadog
      - DD_ENV=public-docker-local-dev
      - DD_LOGS_INJECTION=true
      - DD_SERVICE=web-service-celery-worker
      - DB_USER=postgres
      - DB_PASSWORD=password
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=db
      - CELERY_BROKER_URL=pyamqp://${SERVICE1_RABBIT_MQ_USER}:${SERVICE1_RABBIT_MQ_PASSWORD}@rabbitmq/${SERVICE1_RABBIT_MQ_VHOST}
      - DJANGO_SETTINGS_MODULE=tutorial.settings
      - SNOWPLOW_HOST=snowplow
      - SNOWPLOW_PORT=9090
      - REDIS_CACHE_LOCATION=redis:6379
      - CELERY_TASK_ALWAYS_EAGER=False
      # the following are pulled from the docker .env file
      - SNOWPLOW_ENABLED
      - DD_TRACE_ENABLED
      - REDIS_PASSWORD
      - DD_API_KEY
      - SPLIT_API_KEY
      - SPLIT_REDIS_HOST
      - SPLIT_REDIS_PORT
      - SPLIT_REDIS_PASSWORD
{% endif %}

{% if "webserver-celery-beat" in in_docker %}
  webserver-celery-beat:
    image: rmelickvida/public-example-django:e2cbe80
    command: "celery beat --loglevel=DEBUG -A tutorial"
    depends_on:
      webserver-celery:
        condition: service_started
      rabbitmq:
        condition: service_started
      postgres:
        condition: service_started
    environment:
      - DD_AGENT_HOST=datadog
      - DD_ENV=public-docker-local-dev
      - DD_LOGS_INJECTION=true
      - DD_SERVICE=web-service-celery-beat
      - DB_USER=postgres
      - DB_PASSWORD=password
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=db
      - CELERY_BROKER_URL=pyamqp://${SERVICE1_RABBIT_MQ_USER}:${SERVICE1_RABBIT_MQ_PASSWORD}@rabbitmq/${SERVICE1_RABBIT_MQ_VHOST}
      - DJANGO_SETTINGS_MODULE=tutorial.settings
      - SNOWPLOW_HOST=snowplow
      - SNOWPLOW_PORT=9090
      - REDIS_CACHE_LOCATION=redis:6379
      - CELERY_TASK_ALWAYS_EAGER=False
      # the following are pulled from the docker .env file
      - SNOWPLOW_ENABLED
      - DD_TRACE_ENABLED
      - REDIS_PASSWORD
      - DD_API_KEY
      - SPLIT_API_KEY
      - SPLIT_REDIS_HOST
      - SPLIT_REDIS_PORT
      - SPLIT_REDIS_PASSWORD
{% endif %}

{% if "microservice" in in_docker %}
  microservice:
    image: rmelickvida/public-example-microservice:6989001
    ports:
      - "${MICROSERVICE_EXTERNAL_PORT}:${MICROSERVICE_INTERNAL_PORT}"
    command: ["/bin/sh", "-c", "./manage.py migrate && ddtrace-run manage.py runserver --noreload 0.0.0.0:${MICROSERVICE_INTERNAL_PORT}"]
    depends_on:
      rabbitmq:
        condition: service_started
      postgres:
        condition: service_started
    environment:
      - DD_AGENT_HOST=datadog
      - DD_ENV=public-docker-local-dev
      - DD_LOGS_INJECTION=true
      - DD_SERVICE=microservice
      - DB_USER=root
      - DB_PASSWORD=password
      - DB_HOST=mysql
      - DB_PORT=3306
      - DB_NAME=microservicedb
      - "CELERY_BROKER_URL=pyamqp://${SERVICE2_RABBIT_MQ_USER}:${SERVICE2_RABBIT_MQ_PASSWORD}@rabbitmq/${SERVICE2_RABBIT_MQ_VHOST}"
      - DJANGO_SETTINGS_MODULE=tutorial.settings
      - SNOWPLOW_HOST=snowplow
      - SNOWPLOW_PORT=9090
      - "REDIS_CACHE_LOCATION=redis:6379"
      - CELERY_TASK_ALWAYS_EAGER=${MICROSERVICE_CELERY_TASK_ALWAYS_EAGER}
      # the following are pulled from the docker .env file
      - SNOWPLOW_ENABLED
      - DD_TRACE_ENABLED
      - REDIS_PASSWORD
      - DD_API_KEY
      - SPLIT_API_KEY
      - SPLIT_REDIS_HOST
      - SPLIT_REDIS_PORT
      - SPLIT_REDIS_PASSWORD
{% endif %}

  vida-proxy:
    image: nginx:1.21.4
    volumes:
      - type: bind
        source: ./compose-service-configs/generated/vida-proxy.conf.template
        target: /etc/nginx/templates/default.conf.template
        read_only: true
      - type: bind
        source: ./compose-service-configs/static/vida-proxy/proxy_params
        target: /etc/nginx/proxy_params
        read_only: true
      - type: bind
        source: ./compose-service-configs/static/vida-proxy/upstream_params
        target: /etc/nginx/upstream_params
        read_only: true
    ports:
      - "${VIDA_PROXY_EXTERNAL_PORT}:${VIDA_PROXY_INTERNAL_PORT}"
{% if in_docker | length >= 1 %}
    depends_on:
      # vida-proxy should depend on all other services: it needs to be started last
      # It looks up service name -> ip address during startup, so they all need to be
      # available when it starts.
{% for service in in_docker %}
      - {{ service }}
{% endfor %}
{% endif %}
    # pass all the .env variables, so we don't have to manually specify each service
    env_file: .env

  mysql:
    image: mysql:5.7.36
    volumes:
      - type: volume
        source: mysql-database
        target: /var/lib/mysql
    ports:
      - "${MYSQL_EXTERNAL_PORT}:3306"
    environment:
      MYSQL_ROOT_PASSWORD: "password"

  postgres:
    image: postgres:13.5
    volumes:
      - type: volume
        source: postgres-database
        target: /var/lib/postgresql/data
    user: postgres
    # use the following command if you want to print all queries
    # command: ["postgres", "-c", "log_statement=all"]
    ports:
      - "${POSTGRES_EXTERNAL_PORT}:5432"
    environment:
      POSTGRES_PASSWORD: "password"

{% if "rabbitmq" in in_docker %}
  rabbitmq:
    image: rabbitmq:3.8.26-management-alpine
    ports:
      - ${RABBITMQ_HTTP_EXTERNAL_PORT}:15672
      - ${RABBITMQ_AMQP_EXTERNAL_PORT}:5672
    volumes:
      - type: bind
        source: ./compose-service-configs/static/rabbitmq/rabbitmq.conf
        target: /etc/rabbitmq/rabbitmq.conf
        read_only: true
      - type: bind
        source: ./compose-service-configs/static/rabbitmq/definitions.json
        target: /etc/rabbitmq/definitions.json
        read_only: true
{% endif %}


{% if "redis" in in_docker %}
  redis:
    image: redis:5.0.14-alpine
    # redis defaults to RDB persistence (https://redis.io/topics/persistence)
    # consider using AOF (--appendonly yes) if people still report issues with cached data
    # disappearing after they restart their docker services
    command: ["redis-server", "--requirepass", "${REDIS_PASSWORD}"]
    volumes:
      - type: volume
        source: redis-volume
        target: /data
    ports:
      - ${REDIS_EXTERNAL_PORT}:6379
{% endif %}

{% if "split-synchronizer" in in_docker %}
  split-synchronizer:
    image: splitsoftware/split-synchronizer:4.0.5
    ports:
      - ${SPLIT_SYNCHRONIZER_ADMIN_EXTERNAL_PORT}:3010
    depends_on:
      split-synchronizer-redis:
        condition: service_started
    environment:
      - SPLIT_SYNC_LOG_STDOUT="true"
      - SPLIT_SYNC_IMPRESSIONS_THREADS=1
      - SPLIT_SYNC_IMPRESSIONS_POST_RATE=300
      - SPLIT_SYNC_API_KEY=${SPLIT_API_KEY}
      - SPLIT_SYNC_REDIS_PASS=${SPLIT_REDIS_PASSWORD}
      - SPLIT_SYNC_REDIS_HOST=${SPLIT_REDIS_HOST}
      - SPLIT_SYNC_REDIS_PORT=${SPLIT_REDIS_PORT}

  split-synchronizer-redis:
    image: redis:5.0.14-alpine
    # redis defaults to RDB persistence (https://redis.io/topics/persistence)
    # consider using AOF (--appendonly yes) if people still report issues with cached data
    # disappearing after they restart their docker services
    command: ["redis-server", "--requirepass", "${SPLIT_REDIS_PASSWORD}"]
    volumes:
      - type: volume
        source: split-synchronizer-redis-volume
        target: /data
    ports:
      - ${SPLIT_REDIS_EXTERNAL_PORT}:${SPLIT_REDIS_PORT}
{% endif %}

{% if "snowplow" in in_docker %}
  # Info about to view the events you generated available at https://github.com/snowplow-incubator/snowplow-micro
  snowplow:
    image: snowplow/snowplow-micro:1.2.1
    command: "--collector-config /config/micro.conf --iglu /config/iglu.json"
    ports:
      - ${SNOWPLOW_EXTERNAL_PORT}:9090
    volumes:
      - type: bind
        source: ./compose-service-configs/static/snowplow
        target: /config
        read_only: true
{% endif %}

{% if "datadog" in in_docker %}
  datadog:
    image: gcr.io/datadoghq/agent:7.32.3
    ports:
      - 8125:8125/udp
      - 8126:8126
    environment:
      - DD_APM_ENABLED=true
      - DD_APM_NON_LOCAL_TRAFFIC=true
      - DD_DOGSTATSD_NON_LOCAL_TRAFFIC=true
      - DD_ENV=public-docker-local-dev
      # taken from the .env file
      - DD_API_KEY
      # uncomment the DD_PROCESS_AGENT if you need to enable cpu profiling
      #- DD_PROCESS_AGENT_ENABLED=true
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /etc/passwd:/etc/passwd:ro
      # disable all the host checks
      - type: bind
        source: ./compose-service-configs/static/datadog/conf.d/
        target: /etc/datadog-agent/conf.d/
        read_only: true
    # disable noisy datadog logging
    logging:
      driver: "none"
{% endif %}

networks:
  default:
    name: docker-local-dev

# specify the database directories as volumes, to avoid performance overhead of bind mounts
# https://docs.docker.com/storage/volumes/
# https://github.com/docker/for-mac/issues/4981#issuecomment-733796401
# https://engageinteractive.co.uk/blog/making-docker-faster-on-mac
volumes:
  postgres-database:
  mysql-database:
  redis-volume:
  split-synchronizer-redis-volume:
